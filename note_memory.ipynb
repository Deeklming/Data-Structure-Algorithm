{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationSummaryMemory, ConversationSummaryBufferMemory\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# choose chat\n",
    "chat = ChatOllama(\n",
    "    # model=\"gemma:latest\",\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "def add_message(i, o):\n",
    "    memory.save_context({\"input\":i}, {\"output\":o})\n",
    "\n",
    "## ConversationBuffer Memory\n",
    "memory = ConversationBufferMemory(return_messages=False) #챗모델을 사용할거면 return_messages를 True로 설정\n",
    "# memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationBuffer Window Memory, 가장 최근 몇개만 저장\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=3)\n",
    "\n",
    "# add_message(1,1)\n",
    "# add_message(2,2)\n",
    "# add_message(3,3)\n",
    "# add_message(4,4)\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationSummary Memory, 메세지가 많아지면 요약해서 토큰양을 줄이기 위해 이거 사용\n",
    "memory = ConversationSummaryMemory(llm=chat)\n",
    "# add_message(\"Hi I'm AJin, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationSummary Buffer Memory, 가장 최근거 몇개만 기억하고 그것을 넘어가면 요약해서 저장함\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=50, return_messages=True)\n",
    "# add_message(\"Hi I'm AJin, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "# add_message(\"South Korea is colorful.\", \"I wish I could go! yes!\")\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationKGM(Knowledge Graph) Memory\n",
    "memory = ConversationKGMemory(llm=chat, return_messages=True)\n",
    "# add_message(\"Hi I'm AiJin, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# print(memory.load_memory_variables({\"input\":\"who is AiJin\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory on LLMChain\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=60, memory_key=\"chat_history\")\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You: \n",
    "\"\"\"\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=False # 체인 로그 확인용\n",
    ")\n",
    "# chain.predict(question=\"My name is JungWu.\")\n",
    "# chain.predict(question=\"I live in Seoul.\")\n",
    "# chain.predict(question=\"What is my name?\")\n",
    "\n",
    "## Chat Based Memory\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=100,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human. You speak briefly.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # 누가 보냈는지, 예측하기 어렵고 제한 없는 양의 메세지를 가져 각각 구분함\n",
    "    (\"human\", \"{question}\")    \n",
    "])\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=False # 체인 로그 확인용\n",
    ")\n",
    "# chain.predict(question=\"My name is JungWu\")\n",
    "# chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'My name is Tina.'}\n",
      " Hello Tina, nice to meet you. How can I assist you today?content=' Hello Tina, nice to meet you. How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "## LCEL(LangChain Expression Language) Based Memory, 메모리를 수동적으로 저장 및 사용, 추천\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=100,\n",
    "    return_messages=True\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human. You give short answers.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # 누가 보냈는지, 예측하기 어렵고 제한 없는 양의 메세지를 가져 각각 구분함\n",
    "    (\"human\", \"{question}\")    \n",
    "])\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | chat\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})\n",
    "    print(result)\n",
    "invoke_chain(\"My name is Tina.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is my name?'}\n",
      " The name mentioned earlier was Tina. Is there a specific name you'd like me to use instead? Let me know if that's the case! Otherwise, feel free to ask any question or request assistance with a task. I'm here to help! 😊content=\" The name mentioned earlier was Tina. Is there a specific name you'd like me to use instead? Let me know if that's the case! Otherwise, feel free to ask any question or request assistance with a task. I'm here to help! 😊\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
