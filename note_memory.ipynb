{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama\n",
    "from langchain.chat_models import ChatOllama\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationSummaryMemory, ConversationSummaryBufferMemory\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# choose chat\n",
    "chat = ChatOllama(\n",
    "    # model=\"gemma:latest\",\n",
    "    model=\"mistral:latest\",\n",
    "    temperature=0.1,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "def add_message(i, o):\n",
    "    memory.save_context({\"input\":i}, {\"output\":o})\n",
    "\n",
    "## ConversationBuffer Memory\n",
    "memory = ConversationBufferMemory(return_messages=False) #ì±—ëª¨ë¸ì„ ì‚¬ìš©í• ê±°ë©´ return_messagesë¥¼ Trueë¡œ ì„¤ì •\n",
    "# memory.save_context({\"input\":\"Hi!\"}, {\"output\":\"How are you?\"})\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationBuffer Window Memory, ê°€ì¥ ìµœê·¼ ëª‡ê°œë§Œ ì €ì¥\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=3)\n",
    "\n",
    "# add_message(1,1)\n",
    "# add_message(2,2)\n",
    "# add_message(3,3)\n",
    "# add_message(4,4)\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationSummary Memory, ë©”ì„¸ì§€ê°€ ë§ì•„ì§€ë©´ ìš”ì•½í•´ì„œ í† í°ì–‘ì„ ì¤„ì´ê¸° ìœ„í•´ ì´ê±° ì‚¬ìš©\n",
    "memory = ConversationSummaryMemory(llm=chat)\n",
    "# add_message(\"Hi I'm AJin, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationSummary Buffer Memory, ê°€ì¥ ìµœê·¼ê±° ëª‡ê°œë§Œ ê¸°ì–µí•˜ê³  ê·¸ê²ƒì„ ë„˜ì–´ê°€ë©´ ìš”ì•½í•´ì„œ ì €ì¥í•¨\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=50, return_messages=True)\n",
    "# add_message(\"Hi I'm AJin, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# add_message(\"South Korea is so pretty\", \"I wish I could go!!!\")\n",
    "# add_message(\"South Korea is colorful.\", \"I wish I could go! yes!\")\n",
    "# print(get_history())\n",
    "\n",
    "## ConversationKGM(Knowledge Graph) Memory\n",
    "memory = ConversationKGMemory(llm=chat, return_messages=True)\n",
    "# add_message(\"Hi I'm AiJin, I live in South Korea\", \"Wow that is so cool!\")\n",
    "# print(memory.load_memory_variables({\"input\":\"who is AiJin\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memory on LLMChain\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=60, memory_key=\"chat_history\")\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You: \n",
    "\"\"\"\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=PromptTemplate.from_template(\"{question}\"),\n",
    "    verbose=False # ì²´ì¸ ë¡œê·¸ í™•ì¸ìš©\n",
    ")\n",
    "# chain.predict(question=\"My name is JungWu.\")\n",
    "# chain.predict(question=\"I live in Seoul.\")\n",
    "# chain.predict(question=\"What is my name?\")\n",
    "\n",
    "## Chat Based Memory\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=100,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human. You speak briefly.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # ëˆ„ê°€ ë³´ëƒˆëŠ”ì§€, ì˜ˆì¸¡í•˜ê¸° ì–´ë µê³  ì œí•œ ì—†ëŠ” ì–‘ì˜ ë©”ì„¸ì§€ë¥¼ ê°€ì ¸ ê°ê° êµ¬ë¶„í•¨\n",
    "    (\"human\", \"{question}\")    \n",
    "])\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    memory=memory,\n",
    "    prompt=prompt,\n",
    "    verbose=False # ì²´ì¸ ë¡œê·¸ í™•ì¸ìš©\n",
    ")\n",
    "# chain.predict(question=\"My name is JungWu\")\n",
    "# chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'My name is Tina.'}\n",
      " Hello Tina, nice to meet you. How can I assist you today?content=' Hello Tina, nice to meet you. How can I assist you today?'\n"
     ]
    }
   ],
   "source": [
    "## LCEL(LangChain Expression Language) Based Memory, ë©”ëª¨ë¦¬ë¥¼ ìˆ˜ë™ì ìœ¼ë¡œ ì €ì¥ ë° ì‚¬ìš©, ì¶”ì²œ\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    max_token_limit=100,\n",
    "    return_messages=True\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI talking to a human. You give short answers.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"), # ëˆ„ê°€ ë³´ëƒˆëŠ”ì§€, ì˜ˆì¸¡í•˜ê¸° ì–´ë µê³  ì œí•œ ì—†ëŠ” ì–‘ì˜ ë©”ì„¸ì§€ë¥¼ ê°€ì ¸ ê°ê° êµ¬ë¶„í•¨\n",
    "    (\"human\", \"{question}\")    \n",
    "])\n",
    "def load_memory(input):\n",
    "    print(input)\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "chain = RunnablePassthrough.assign(history=load_memory) | prompt | chat\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    memory.save_context({\"input\":question}, {\"output\":result.content})\n",
    "    print(result)\n",
    "invoke_chain(\"My name is Tina.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is my name?'}\n",
      " The name mentioned earlier was Tina. Is there a specific name you'd like me to use instead? Let me know if that's the case! Otherwise, feel free to ask any question or request assistance with a task. I'm here to help! ğŸ˜Šcontent=\" The name mentioned earlier was Tina. Is there a specific name you'd like me to use instead? Let me know if that's the case! Otherwise, feel free to ask any question or request assistance with a task. I'm here to help! ğŸ˜Š\"\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"What is my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
